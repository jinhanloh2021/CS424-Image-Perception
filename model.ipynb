{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# SMU Logo bounding box detection\n",
            "Design an algorithm to identify all images of SMU from a set of random images. A bounding box should be drawn around the SMU logo whenever it appears in an image.\n",
            "\n",
            "When an SMU logo occurs in an image, a bounding box should be drawn around it. Logo identification will be evaluated by its F1 score."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Setup\n",
            "Install dependencies\n",
            "```bash\n",
            "pip install -r requirements.txt\n",
            "\n",
            "```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "%pip install -r requirements.txt"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Import packages"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "from ultralytics import YOLO\n",
            "from roboflow import Roboflow\n",
            "from PIL import Image\n",
            "from dotenv import load_dotenv\n",
            "from manage_model import update_path, convert_bbox_format, get_f1\n",
            "import os"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Load environment variables using dotenv\n",
            "- `API_KEY` - API Key for Roboflow dataset\n",
            "- `DATA_VERSION` - Version of dataset used\n",
            "- `PATH_TO_DATA` - Absolute file path to the \"SMU-Logo-Detection-1\" folder. See .env.example for example"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "C:/Users/jinha/Documents/CS424/Project SMU Logo/tree-type-detection-11\n"
               ]
            }
         ],
         "source": [
            "# Project settings\n",
            "load_dotenv()\n",
            "api_key = os.getenv('API_KEY')\n",
            "data_version = os.getenv('DATA_VERSION')\n",
            "path_to_project = os.getenv(\"PATH_TO_PROJECT\")\n",
            "\n",
            "# If on SMU Violet, can't load env variables. Write manually.\n",
            "\n",
            "path_to_data = f'{path_to_project}/tree-type-detection-{data_version}'\n",
            "print(path_to_data)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Load dataset into workspace"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "loading Roboflow workspace...\n",
                  "loading Roboflow project...\n",
                  "Data version: 11\n"
               ]
            }
         ],
         "source": [
            "rf = Roboflow(api_key=api_key)\n",
            "project = rf.workspace(\"tree-dataset-iftyz\").project(\"tree-type-detection-9rfxy\")\n",
            "dataset = project.version(data_version).download(\"yolov8\")\n",
            "print(f'Data version: {data_version}')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Update test, train and val values in data.yaml\n",
            "This fixes a file not found bug"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "update_path(data_version, path_to_project)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### [Training](https://docs.ultralytics.com/modes/train/#train-settings)\n",
            "Use a trained model, saved in `/saved_models`, or train a model.\n",
            "A pre-trained [model](https://github.com/ultralytics/ultralytics?tab=readme-ov-file) `yolov8n.pt` from Ultralytics should be used. Tune hyper-params such as learning rate and epochs.\n",
            "\n",
            "If there is file not found error, check the `FILE_TO_PATH` env variable, and make sure that you've updated the `data.yml` to the absolute file path."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "New https://pypi.org/project/ultralytics/8.1.24 available  Update with 'pip install -U ultralytics'\n",
                  "Ultralytics YOLOv8.0.196  Python-3.11.2 torch-2.2.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 6144MiB)\n",
                  "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=D:/Disk D Documents/CS424/Project SMU Logo/tree-type-detection-9/data.yaml, epochs=16, patience=2, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train6\n",
                  "Overriding model.yaml nc=80 with nc=1\n",
                  "\n",
                  "                   from  n    params  module                                       arguments                     \n",
                  "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
                  "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
                  "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
                  "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
                  "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
                  "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
                  "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
                  "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
                  "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
                  "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
                  " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                  " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                  " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
                  " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                  " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                  " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
                  " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
                  " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                  " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
                  " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
                  " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                  " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
                  " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
                  "Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n",
                  "\n",
                  "Transferred 349/355 items from pretrained weights\n",
                  "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train6', view at http://localhost:6006/\n",
                  "Freezing layer 'model.22.dfl.conv.weight'\n",
                  "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Disk D Documents\\CS424\\Project SMU Logo\\tree-type-detection-9\\train\\labels... 3455 images, 2216 backgrounds, 0 corrupt: 100%|██████████| 3455/3455 [00:11<00:00, 296.19it/s]\n",
                  "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Disk D Documents\\CS424\\Project SMU Logo\\tree-type-detection-9\\train\\labels.cache\n",
                  "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Disk D Documents\\CS424\\Project SMU Logo\\tree-type-detection-9\\valid\\labels... 764 images, 635 backgrounds, 0 corrupt: 100%|██████████| 764/764 [00:03<00:00, 235.77it/s]\n",
                  "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Disk D Documents\\CS424\\Project SMU Logo\\tree-type-detection-9\\valid\\labels.cache\n",
                  "Plotting labels to runs\\detect\\train6\\labels.jpg... \n",
                  "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
                  "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
                  "Image sizes 640 train, 640 val\n",
                  "Using 4 dataloader workers\n",
                  "Logging results to \u001b[1mruns\\detect\\train6\u001b[0m\n",
                  "Starting training for 16 epochs...\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       1/16      6.72G      1.753      5.946      1.246          9        640: 100%|██████████| 216/216 [06:47<00:00,  1.89s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:38<00:00,  1.59s/it]\n",
                  "                   all        764        149      0.702      0.416      0.438      0.241\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       2/16      6.77G      1.896      1.807      1.342          3        640: 100%|██████████| 216/216 [06:37<00:00,  1.84s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:32<00:00,  1.37s/it]\n",
                  "                   all        764        149      0.769      0.443      0.473      0.266\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       3/16      6.77G      1.854      1.782      1.307          3        640: 100%|██████████| 216/216 [04:27<00:00,  1.24s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:38<00:00,  1.61s/it]\n",
                  "                   all        764        149      0.844       0.45      0.507      0.302\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       4/16      6.77G      1.782      1.615      1.303         12        640: 100%|██████████| 216/216 [04:32<00:00,  1.26s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:36<00:00,  1.54s/it]\n",
                  "                   all        764        149      0.751      0.537      0.556       0.32\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       5/16      6.77G      1.681      1.444       1.27          9        640: 100%|██████████| 216/216 [04:29<00:00,  1.25s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:38<00:00,  1.60s/it]\n",
                  "                   all        764        149      0.827      0.503      0.558       0.29\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       6/16      6.77G      1.579      1.263      1.196         15        640: 100%|██████████| 216/216 [04:47<00:00,  1.33s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:36<00:00,  1.54s/it]\n",
                  "                   all        764        149      0.831      0.629      0.668      0.423\n",
                  "Closing dataloader mosaic\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       7/16      6.77G      1.533      1.175      1.218          3        640: 100%|██████████| 216/216 [04:05<00:00,  1.14s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:34<00:00,  1.42s/it]\n",
                  "                   all        764        149      0.832      0.584      0.644      0.391\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       8/16      6.77G      1.455       1.14      1.176          7        640: 100%|██████████| 216/216 [04:10<00:00,  1.16s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:32<00:00,  1.36s/it]\n",
                  "                   all        764        149       0.89      0.596      0.672      0.424\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "       9/16      6.77G      1.435      1.092      1.141          7        640: 100%|██████████| 216/216 [04:38<00:00,  1.29s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:33<00:00,  1.41s/it]\n",
                  "                   all        764        149      0.826      0.611      0.644      0.411\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "      10/16      6.77G      1.321     0.9063      1.092          7        640: 100%|██████████| 216/216 [04:40<00:00,  1.30s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:32<00:00,  1.35s/it]\n",
                  "                   all        764        149      0.835      0.677      0.684      0.458\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "      11/16      6.77G      1.272     0.8735      1.076          6        640: 100%|██████████| 216/216 [04:44<00:00,  1.32s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:33<00:00,  1.40s/it]\n",
                  "                   all        764        149      0.932      0.604      0.689      0.509\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "      12/16      6.77G      1.229     0.8419      1.055          9        640: 100%|██████████| 216/216 [05:32<00:00,  1.54s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:32<00:00,  1.36s/it]\n",
                  "                   all        764        149      0.896      0.658       0.71      0.471\n",
                  "\n",
                  "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
                  "      13/16      6.77G       1.17      0.756      1.024          3        640: 100%|██████████| 216/216 [05:29<00:00,  1.52s/it]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:33<00:00,  1.40s/it]\n",
                  "                   all        764        149      0.867      0.664       0.72        0.5\n",
                  "Stopping training early as no improvement observed in last 2 epochs. Best results observed at epoch 11, best model saved as best.pt.\n",
                  "To update EarlyStopping(patience=2) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
                  "\n",
                  "13 epochs completed in 1.222 hours.\n",
                  "Optimizer stripped from runs\\detect\\train6\\weights\\last.pt, 22.5MB\n",
                  "Optimizer stripped from runs\\detect\\train6\\weights\\best.pt, 22.5MB\n",
                  "\n",
                  "Validating runs\\detect\\train6\\weights\\best.pt...\n",
                  "Ultralytics YOLOv8.0.196  Python-3.11.2 torch-2.2.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 6144MiB)\n",
                  "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 24/24 [00:31<00:00,  1.33s/it]\n",
                  "                   all        764        149      0.932      0.604      0.689      0.509\n",
                  "Speed: 0.3ms preprocess, 39.0ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
                  "Results saved to \u001b[1mruns\\detect\\train6\u001b[0m\n"
               ]
            }
         ],
         "source": [
            "# # Train model (Laptop)\n",
            "# model = YOLO('yolov8n.pt') # pre-trained yolov8 nano\n",
            "# results = model.train(data=f'{path_to_data}/data.yaml', epochs=10)\n",
            "\n",
            "# # Train model (SMU GPU)\n",
            "model = YOLO('yolov8s.pt')\n",
            "results = model.train(data=f'{path_to_data}/data.yaml', epochs=16, device='0', patience = 2, amp=False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Use saved model\n",
            "model = YOLO('./saved_models/v11_epoch16.pt')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "After training, the model can be found in `/runs`. Save the `best.pt` model in the `/saved_models` folder."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### [Validation](https://docs.ultralytics.com/modes/val/#usage-examples)\n",
            "Val mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters of the model to improve its performance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Ultralytics YOLOv8.0.196  Python-3.10.6 torch-2.2.1+cpu CPU (AMD Ryzen 9 5900HX with Radeon Graphics)\n",
                  "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
                  "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\tree-type-detection-11\\valid\\labels.cache... 739 images, 610 backgrounds, 0 corrupt: 100%|██████████| 739/739 [00:00<?, ?it/s]\n",
                  "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 47/47 [06:47<00:00,  8.67s/it]\n",
                  "                   all        739        149        0.9      0.725      0.834      0.645\n",
                  "Speed: 2.6ms preprocess, 539.8ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
                  "Results saved to \u001b[1mruns\\detect\\val12\u001b[0m\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "'\\n1. Epochs: Vary from 7, 15 etc\\n2. Find a way to output the predicted labels into a folder. Compare the predicted labels and actual labels to check the metrics\\n3. Re-annotate if have time (those with very small logos)\\n\\n\\n4. annotate the rest of the images\\n5. after everything is annotated, augmentation and final training\\n'"
                  ]
               },
               "execution_count": 16,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "validation_results = model.val(data=f'{path_to_data}/data.yaml', conf=0.25, iou=0.6)\n",
            "'''\n",
            "1. Epochs: Vary from 7, 15 etc\n",
            "2. Find a way to output the predicted labels into a folder. Compare the predicted labels and actual labels to check the metrics\n",
            "3. Re-annotate if have time (those with very small logos)\n",
            "\n",
            "\n",
            "4. annotate the rest of the images\n",
            "5. after everything is annotated, augmentation and final training\n",
            "'''"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "0.8029739776951672"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "get_f1(validation_results.results_dict)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### [Test](https://docs.ultralytics.com/modes/predict/#why-use-ultralytics-yolo-for-inference)\n",
            "Predict bounding boxes on test set"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n",
                  "image 1/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1293.JPG: 448x640 (no detections), 122.0ms\n",
                  "image 2/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1319.JPG: 448x640 (no detections), 126.0ms\n",
                  "image 3/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1677.JPG: 640x448 (no detections), 150.0ms\n",
                  "image 4/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1708.JPG: 640x448 (no detections), 127.0ms\n",
                  "image 5/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1722.JPG: 448x640 (no detections), 117.0ms\n",
                  "image 6/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1761.JPG: 384x640 2 SMU-Logos, 102.0ms\n",
                  "image 7/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1897.JPG: 640x544 3 SMU-Logos, 139.0ms\n",
                  "image 8/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1912.JPG: 448x640 1 SMU-Logo, 125.0ms\n",
                  "image 9/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\DSCF1919.JPG: 448x640 (no detections), 120.0ms\n",
                  "image 10/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\sample1.jpg: 480x640 (no detections), 125.0ms\n",
                  "image 11/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\sample2.jpg: 640x640 (no detections), 167.0ms\n",
                  "image 12/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\sample3.jpg: 448x640 1 SMU-Logo, 127.0ms\n",
                  "image 13/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\wrong1.jpg: 448x640 (no detections), 139.0ms\n",
                  "image 14/14 C:\\Users\\jinha\\Documents\\CS424\\Project SMU Logo\\prof_images\\wrong2.jpg: 480x640 (no detections), 135.0ms\n",
                  "Speed: 2.9ms preprocess, 130.1ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n"
               ]
            }
         ],
         "source": [
            "# Predict\n",
            "# prediction = model.predict(source=f\"{path_to_data}/test/images\", conf=0.25, iou=0.6)\n",
            "prediction = model.predict(source=f\"C:/Users/jinha/Documents/CS424/Project SMU Logo/prof_images\", conf=0.1, iou=0.6)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Save images to `/predictions` folder if bounding box exists."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "4"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "images = {}\n",
            "\n",
            "if not os.path.exists(f'{path_to_data}/../predictions'):\n",
            "    os.makedirs(f'{path_to_data}/../predictions')\n",
            "\n",
            "for i in range(0, len(prediction)):\n",
            "    if prediction[i].boxes.shape[0] >= 1:\n",
            "        im_rgb = Image.fromarray(prediction[i].plot(line_width=1)[..., ::-1])\n",
            "        images[i] = im_rgb  # RGB-order PIL image\n",
            "        predicted_box = convert_bbox_format(prediction[i])\n",
            "\n",
            "        # Write labels\n",
            "        with open(f'{path_to_data}/../predictions/obj_est.txt', 'a') as f:\n",
            "            f.write(', '.join(map(str, predicted_box)) + '\\n')\n",
            "        # Save images\n",
            "        im_rgb.save(fp=f'{path_to_data}/../predictions/{predicted_box[0]}_labelled.jpg')\n",
            "\n",
            "len(images)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
